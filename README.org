#+title: Vibecut: a vibe-coded video editor

* What is this?
This is a long-term experiment to see how viable vibe-coding with AI
agents is in 2026.

* The Goal
Create an advanced desktop application for video editing. Simpler than
Final Cut, but more advanced than iMovie. We want to use minimal
amount of manual code typing, and prefer written or spoken prompts.

* Tech Stack
- GNU+Linux
- C++23
- CMake
- GLFW + OpenGL 4
- ImGui
- ffmpeg

* Building

** Dependencies

Install GLFW on your Linux distribution:

#+begin_src bash
# Arch Linux
sudo pacman -S glfw

# Ubuntu/Debian
sudo apt install libglfw3-dev

# Fedora
sudo dnf install glfw-devel
#+end_src

** Clone and Build

#+begin_src bash
git clone --recurse-submodules https://github.com/lszl84/vibecut.git
cd vibecut
cmake -B build
cmake --build build
./build/vibecut
#+end_src

If you already cloned without submodules:

#+begin_src bash
git submodule update --init
#+end_src

* Time Tracking

#+BEGIN: clocktable :maxlevel nil :scope file :block nil
#+CAPTION: Clock summary at [2026-01-17 Sat 15:47]
| Headline          | Time  |       |
|-------------------+-------+-------|
| *Total time*        | *11:51* |       |
|-------------------+-------+-------|
| Tasks             | 11:51 |       |
| \_  Initial Setup |       |  0:13 |
| \_  Vibe coding   |       | 10:45 |
| \_  Manual coding |       |  0:53 |
#+END:

* Tasks
We'll track the time spent on the tasks below:
** Initial Setup
Git Setup, AGENTS.md, workflow changes, etc.
:LOGBOOK:
CLOCK: [2026-01-04 Sun 15:10]--[2026-01-04 Sun 15:16] =>  0:06
CLOCK: [2026-01-04 Sun 14:57]--[2026-01-04 Sun 15:04] =>  0:07
:END:
** Vibe coding
:LOGBOOK:
CLOCK: [2026-01-17 Sat 13:52]--[2026-01-17 Sat 15:47] =>  1:55
CLOCK: [2026-01-16 Fri 09:21]--[2026-01-16 Fri 10:27] =>  1:06
CLOCK: [2026-01-16 Fri 09:21]--[2026-01-16 Fri 09:21] =>  0:00
CLOCK: [2026-01-15 Thu 12:00]--[2026-01-15 Thu 12:23] =>  0:23
CLOCK: [2026-01-14 Wed 08:40]--[2026-01-14 Wed 09:37] =>  0:57
CLOCK: [2026-01-13 Tue 18:14]--[2026-01-13 Tue 20:29] =>  2:15
CLOCK: [2026-01-11 Sun 10:04]--[2026-01-11 Sun 11:15] =>  1:11
CLOCK: [2026-01-10 Sat 12:43]--[2026-01-10 Sat 13:28] =>  0:45
CLOCK: [2026-01-09 Fri 17:45]--[2026-01-09 Fri 18:40] =>  0:55
CLOCK: [2026-01-08 Thu 19:40]--[2026-01-08 Thu 20:14] =>  0:34
CLOCK: [2026-01-06 Tue 19:22]--[2026-01-06 Tue 20:03] =>  0:41
CLOCK: [2026-01-04 Sun 15:19]--[2026-01-04 Sun 15:22] =>  0:03
:END:
Typing or speaking prompts, babysiting Agents, testing the features
implemented by agents, etc.
** Manual coding
:LOGBOOK:
CLOCK: [2026-01-16 Fri 09:06]--[2026-01-16 Fri 09:21] =>  0:15
CLOCK: [2026-01-14 Wed 09:54]--[2026-01-14 Wed 10:32] =>  0:38
:END:
Adding/changing code by hand, debugging, etc.

* Devlog

Live Streams: https://www.youtube.com/@lukeatdevmindscape/streams

** Day 11
Good progress, added audio, fixed clips reordering with improved UI,
and started building the infrastructure for project management and
inserting external clips to the timeline.

I need to think about the UX of that feature, probably take a closer
look at what Final Cut does.

** Day 10
So I set out to fix the export issue, started a new ffmpeg project
(with the help of AI of course), and then I noticed that it wasn't our
export function that's incorrect!

It turned out the bug is in the video loading for weird fps. So that
was fixed pretty quickly by AI (I moved to the recently released Codex
5.2 from Claude Opus 4.5 btw). That's a nice win, because I want to
keep the manual coding to the minimum in this project, so that we can
really test how viable pure or almost pure vibe coding is.

After fixing the decoding we moved on to clip reordering, which is a
huge UX change from the architectural POV. Codex struggled a bit, but
was able to create someting more or less working in like 40 minutes,
so not bad overall.

Tomorrow we will continue fixing the remaining bugs. The next big
feature is loading other videos to the project, so that we can add
more clips to the timeline. This will be a huge feature, moving us
closer towards a working NLE.

** Day 9
This is just annoying at this point. After reviewing the code and
debugging it a little bit, I decided to give Claude one last chance to
fix the bug, because I realized it's gonna take me a lot of time to
learn libAV API to be able to fix the problem manually.

But the LLM failed again and now I don't see any other choice than to
learn ffmpeg, experiment with it, and write my own encoding logic.
This will probably take me a few days, so I will be back with live
streams when I'm ready for vibe coding again.

** Day 8
This session was so disappointing. One hour of back and forth and
Claude still wasn't able to do a proper export. It wrote an export to
PNG frames, and that export function is correct, but it wasn't able to
map that to a correct MP4 export without missing frames.

I guess I'll have to fix it manually. Let's see how much time it takes
me.

UPDATE: I did some manual debugging and added TODO notes to check with
Claude tomorrow.

** Day 7
This one was pretty interesting. Claude could not seem to fix various
problem with frame-by-frame navigation, until I suggested using
integer-based frame indexing to avoid floating point errors.

This is not something that a non-programmer would be able to come up
with, so I think we're still not ready for full-blown vibe coding,
where regular people can create complex software.

At least not as of 13 Jan 2026 ;)

After switching to integer-based indexing, Claude managed to fix some
off-by-one errors and we made navigation between frames resemble more
what FinalCut does. Still, there are some errors with exporting
(missing frames?) - not sure if this is related to how video
re-encoding works in general, or a bug in our code.

** Day 6
Added keyboard navigation between clips and frames, timeline zoom with
gestures and a scrollbar, turned the timeline into FinalCut-like
"magnetic timeline".

Overall a good progress, with some minor bugs introduced along the
way. I now have confidence in Claude that it will be able to fix these
in the following sessions.

** Day 5
Claude blew my mind again. It fixed all the issues, and made the UI
much prettier just by basically following a prompt like "make it more
modern, like Final Cut or sth."

Unbelievable stuff.

The video editor is now starting to be actually useful. You can break
a clip at the playhead by hitting B, and you can adjust the clip start
and endpoints by dragging the clip handles. Then export the video.

** Day 4
Claude managed to fix the bug after a few back-and-forth prompts.
Think about this: with absolutely no visual feedback, no ability to
play with the UI and see the effect of the changes, it managed to find
and solve the problem...

The process was very tedious, though: basically 30mins of me testing
this stuff and telling the AI it did not work. Are the QA Engineers of
today the future AI Orchestrators?

The next feature to implement was the ability to split the timeline
into multiple clips. The AI understood the feature description
perfectly. There are still bugs that we are going to tackle tomorrow,
but this is quite mind-blowing, to be honest.

** Day 3
Claude integrated FFmpeg C API into the program, created an ImGui
OpenGL video player and added an ability to trim the timeline and
export the video. This is absolutely insane.

However, it got stuck with a bug - wrong behavior when clicking near
the beginning of a timeline for short videos. I wonder if it's
possible to fix this just by prompts? How to explain the issue without
shoving it a recording of the problem? Is it possible to upload a
video as a problem description? Will AI be able to interpret a video?

** Day 2
Just a quick session, this time with live streaming on YouTube. Added
a custom file selector on button click.

** Day 1
Set up the GitHub project with README.org and AGENTS.mg. Added basic
CMakeLists.txt and program code showing a window with a button.
