#+title: Vibecut: a vibe-coded video editor

* What is this?
This is a long-term experiment to see how viable vibe-coding with AI
agents is in 2026.

* The Goal
Create an advanced desktop application for video editing. Simpler than
Final Cut, but more advanced than iMovie. We want to use minimal
amount of manual code typing, and prefer written or spoken prompts.

* Tech Stack
- GNU+Linux
- C++23
- CMake
- GLFW + OpenGL 4
- ImGui
- ffmpeg

* Building

** Dependencies

Install GLFW on your Linux distribution:

#+begin_src bash
# Arch Linux
sudo pacman -S glfw

# Ubuntu/Debian
sudo apt install libglfw3-dev

# Fedora
sudo dnf install glfw-devel
#+end_src

** Clone and Build

#+begin_src bash
git clone --recurse-submodules https://github.com/lszl84/vibecut.git
cd vibecut
cmake -B build
cmake --build build
./build/vibecut
#+end_src

If you already cloned without submodules:

#+begin_src bash
git submodule update --init
#+end_src

* Tasks
We'll track the time spent on the tasks below:
** Initial Setup
Git Setup, AGENTS.md, workflow changes, etc.
:LOGBOOK:
CLOCK: [2026-01-04 Sun 15:10]--[2026-01-04 Sun 15:16] =>  0:06
CLOCK: [2026-01-04 Sun 14:57]--[2026-01-04 Sun 15:04] =>  0:07
:END:
** Vibe coding
:LOGBOOK:
CLOCK: [2026-01-09 Fri 17:45]--[2026-01-09 Fri 18:40] =>  0:55
CLOCK: [2026-01-08 Thu 19:40]--[2026-01-08 Thu 20:14] =>  0:34
CLOCK: [2026-01-06 Tue 19:22]--[2026-01-06 Tue 20:03] =>  0:41
CLOCK: [2026-01-04 Sun 15:19]--[2026-01-04 Sun 15:22] =>  0:03
:END:
Typing or speaking prompts, babysiting Agents, testing the features
implemented by agents, etc.
** Manual coding
Adding/changing code by hand, debugging, etc.

* Time Tracking

#+BEGIN: clocktable :maxlevel nil :scope file :block nil
#+CAPTION: Clock summary at [2026-01-09 Fri 18:44]
| Headline          | Time |      |
|-------------------+------+------|
| *Total time*        | *2:26* |      |
|-------------------+------+------|
| Tasks             | 2:26 |      |
| \_  Initial Setup |      | 0:13 |
| \_  Vibe coding   |      | 2:13 |
#+END:

* Devlog

Live Streams: https://www.youtube.com/@lukeatdevmindscape/streams

** Day 4
Claude managed to fix the bug after a few back-and-forth prompts.
Think about this: with absolutely no visual feedback, no ability to
play with the UI and see the effect of the changes, it managed to find
and solve the problem...

The process was very tedious, though: basically 30mins of me testing
this stuff and telling the AI it did not work. Are the QA Engineers of
today the future AI Orchestrators?

The next feature to implement was the ability to split the timeline
into multiple clips. The AI understood the feature description
perfectly. There are still bugs that we are going to tackle tomorrow,
but this is quite mind-blowing, to be honest.

** Day 3
Claude integrated FFmpeg C API into the program, created an ImGui
OpenGL video player and added an ability to trim the timeline and
export the video. This is absolutely insane.

However, it got stuck with a bug - wrong behavior when clicking near
the beginning of a timeline for short videos. I wonder if it's
possible to fix this just by prompts? How to explain the issue without
shoving it a recording of the problem? Is it possible to upload a
video as a problem description? Will AI be able to interpret a video?

** Day 2
Just a quick session, this time with live streaming on YouTube. Added
a custom file selector on button click.

** Day 1
Set up the GitHub project with README.org and AGENTS.mg. Added basic
CMakeLists.txt and program code showing a window with a button.
